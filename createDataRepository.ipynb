{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This script will read all the data from the data/ThOpt/ folder\n",
    "Then it will create an uniform csv file containing all the data.\n",
    "\n",
    "Each segment will create some pseudo master files.\n",
    "(e.g. master_engin_new_06_09_2016_12_33  \n",
    "master_engin_old_06_09_2016_12_33  \n",
    "master_kemal_06_09_2016_12_30  \n",
    "master_zulkar_6_16_06_09_2016_12_33)\n",
    "\n",
    "all those files are marged to make \"master_<datetime>.csv\" file.\n",
    "\n",
    "The output will be a \"master_<datetime>.csv\"\n",
    "Columns in 'master.csv' file are :\n",
    "file_size - Bytes (int)\n",
    "number_of_files - counts\n",
    "bandwidth - MegaByte (int)\n",
    "rtt - millisecond (int)\n",
    "buffer_size - Bytes \n",
    "p - count (bound <= 32)\n",
    "cc - count (bound <= 32)\n",
    "pp - count (bound <= 32)\n",
    "fast - boolean {0,1}\n",
    "throughput - MegaByte Per Second\n",
    "time - Seconds\n",
    "start_time - yyyy-mm-dd HH:MM:SS (e.g. 2015-03-31 00:57:22)\n",
    "source - string name\n",
    "destination - string name\n",
    "\n",
    "always open it in append mode if you want to add extra data in it. \n",
    "\n",
    "Author: Md S Q Zulkar Nine   Start Date : June 8, 2016    Last Modified: June 10, 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas library:\n",
    "import pandas as pds\n",
    "import pylab as pyl\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>body {\n",
       "    margin: 0;\n",
       "    font-family: Helvetica;\n",
       "}\n",
       "table.dataframe {\n",
       "    border-collapse: collapse;\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe tr {\n",
       "    border: none;\n",
       "}\n",
       "table.dataframe td, table.dataframe th {\n",
       "    margin: 0;\n",
       "    border: 1px solid white;\n",
       "    padding-left: 0.25em;\n",
       "    padding-right: 0.25em;\n",
       "}\n",
       "table.dataframe th:not(:empty) {\n",
       "    background-color: #fec;\n",
       "    text-align: left;\n",
       "    font-weight: normal;\n",
       "}\n",
       "table.dataframe tr:nth-child(2) th:empty {\n",
       "    border-left: none;\n",
       "    border-right: 1px dashed #888;\n",
       "}\n",
       "table.dataframe td {\n",
       "    border: 2px solid #ccf;\n",
       "    background-color: #f4f4ff;\n",
       "}h3 {\n",
       "    color: white;\n",
       "    background-color: black;\n",
       "    padding: 0.5em;\n",
       "}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretty dataframe :\n",
    "from IPython.core.display import HTML\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the file list : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sg5-25M.csv', 'sg0.25-1M.csv', 'sg1G.csv', 'sg3G.csv', 'sg100M.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/kemal/xsede/'\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for Kemal's data: xsede - xsede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>number_of_files</th>\n",
       "      <th>bandwidth</th>\n",
       "      <th>rtt</th>\n",
       "      <th>buffer_size</th>\n",
       "      <th>p</th>\n",
       "      <th>cc</th>\n",
       "      <th>pp</th>\n",
       "      <th>fast</th>\n",
       "      <th>throughput</th>\n",
       "      <th>time</th>\n",
       "      <th>start_time</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>104857664</td>\n",
       "      <td>64</td>\n",
       "      <td>1280</td>\n",
       "      <td>40</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>139.691700</td>\n",
       "      <td>45.815259</td>\n",
       "      <td>2014-11-05 19:36:04</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Gordon/Oasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>104857664</td>\n",
       "      <td>64</td>\n",
       "      <td>1280</td>\n",
       "      <td>40</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62.012022</td>\n",
       "      <td>103.205902</td>\n",
       "      <td>2014-11-05 19:36:50</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Gordon/Oasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104857664</td>\n",
       "      <td>64</td>\n",
       "      <td>1280</td>\n",
       "      <td>40</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>94.532597</td>\n",
       "      <td>67.701626</td>\n",
       "      <td>2014-11-05 19:38:33</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Gordon/Oasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104857664</td>\n",
       "      <td>64</td>\n",
       "      <td>1280</td>\n",
       "      <td>40</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>171.529367</td>\n",
       "      <td>37.311452</td>\n",
       "      <td>2014-11-05 19:42:35</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Gordon/Oasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>104857664</td>\n",
       "      <td>64</td>\n",
       "      <td>1280</td>\n",
       "      <td>40</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>60.514904</td>\n",
       "      <td>105.759175</td>\n",
       "      <td>2014-11-05 19:43:12</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Gordon/Oasis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_size  number_of_files  bandwidth rtt  buffer_size  p  cc  pp  fast  \\\n",
       "0  104857664               64       1280  40     33554432  1   1   1     1   \n",
       "1  104857664               64       1280  40     33554432  1   1   1     0   \n",
       "2  104857664               64       1280  40     33554432  1   1   2     1   \n",
       "3  104857664               64       1280  40     33554432  1   1   4     1   \n",
       "4  104857664               64       1280  40     33554432  1   1   4     0   \n",
       "\n",
       "   throughput        time          start_time    source   destination  \n",
       "0  139.691700   45.815259 2014-11-05 19:36:04  Stampede  Gordon/Oasis  \n",
       "1   62.012022  103.205902 2014-11-05 19:36:50  Stampede  Gordon/Oasis  \n",
       "2   94.532597   67.701626 2014-11-05 19:38:33  Stampede  Gordon/Oasis  \n",
       "3  171.529367   37.311452 2014-11-05 19:42:35  Stampede  Gordon/Oasis  \n",
       "4   60.514904  105.759175 2014-11-05 19:43:12  Stampede  Gordon/Oasis  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_kemal_xsede_' + current_time_str\n",
    "\n",
    "# remove milliseconds from the time:\n",
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%Y-%m-%d:%H:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "header_flag = True\n",
    "for file_name in file_names:\n",
    "    full_file_name = file_dir + file_name\n",
    "    \n",
    "    # read the csv file : \n",
    "    data = pds.read_csv(full_file_name)\n",
    "    \n",
    "    # remove wierd last column :\n",
    "    del data['Unnamed: 15']\n",
    "    \n",
    "    # Merge the date and time column: \n",
    "    data['start_time'] = data['date'] +':'+ data['stime']\n",
    "    \n",
    "    # delete Time and Date column :\n",
    "    del data['stime']\n",
    "    del data['date']\n",
    "    \n",
    "    # reordering the columns inside the dataframe:\n",
    "    column_order = ['file_size', 'number_of_files', 'bandwidth', 'rtt', 'buffer_size', \n",
    "                'p', 'cc', 'pp', 'fast', 'throughput', 'time', 'start_time', 'source', 'destination']\n",
    "    data = data[column_order]\n",
    "    \n",
    "    # remove millisecond from the time : \n",
    "    temp = data.start_time.str.split('.')\n",
    "    temp = temp.apply(remove_miliseconds)\n",
    "    data['start_time']=temp\n",
    "    \n",
    "    \n",
    "    # change the current string date time to Timestamp: \n",
    "    data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "    \n",
    "    \n",
    "    # Unit conversion:\n",
    "    # file_size - Byte\n",
    "    # bandwidth - MegaByte per second\n",
    "    # buffer_size - Byte\n",
    "    # rtt - millisecond\n",
    "    # throughput - MegaByte per second\n",
    "    # time - second \n",
    "    # start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "    #d = data.copy(deep=True)\n",
    "    data['file_size'] = (data['file_size'])\n",
    "    data['bandwidth'] = int(1280)\n",
    "    data['buffer_size'] = (data['buffer_size'] )\n",
    "    data['rtt'] = '40'\n",
    "    data['throughput'] = data['throughput'] / 8 \n",
    "    \n",
    "    # filtering data based on throughput : achievable throughput has shown less then 90% of the bandwidth.\n",
    "    th_filter = data.throughput <  ( data.bandwidth * 0.90)\n",
    "    data = data[th_filter]\n",
    "    \n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for Kemal's data: Local to Local: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lt_100M.csv', 'lt_3-15M.csv', 'lt_3G.csv', 'lt_1G.csv', 'lt_0.25-1M.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/kemal/local/'\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>number_of_files</th>\n",
       "      <th>bandwidth</th>\n",
       "      <th>rtt</th>\n",
       "      <th>buffer_size</th>\n",
       "      <th>p</th>\n",
       "      <th>cc</th>\n",
       "      <th>pp</th>\n",
       "      <th>fast</th>\n",
       "      <th>throughput</th>\n",
       "      <th>time</th>\n",
       "      <th>start_time</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>256036</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4194304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50.498704</td>\n",
       "      <td>4.835340</td>\n",
       "      <td>2014-11-08 00:38:03</td>\n",
       "      <td>Evenstar</td>\n",
       "      <td>Didclab-ws10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>256036</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4194304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36.190913</td>\n",
       "      <td>6.746920</td>\n",
       "      <td>2014-11-08 00:38:08</td>\n",
       "      <td>Evenstar</td>\n",
       "      <td>Didclab-ws10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>256036</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4194304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>42.996127</td>\n",
       "      <td>5.679057</td>\n",
       "      <td>2014-11-08 00:38:14</td>\n",
       "      <td>Evenstar</td>\n",
       "      <td>Didclab-ws10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>256036</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4194304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>41.912495</td>\n",
       "      <td>5.825888</td>\n",
       "      <td>2014-11-08 00:38:20</td>\n",
       "      <td>Evenstar</td>\n",
       "      <td>Didclab-ws10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>256036</td>\n",
       "      <td>1000</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4194304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>47.204078</td>\n",
       "      <td>5.172809</td>\n",
       "      <td>2014-11-08 00:38:26</td>\n",
       "      <td>Evenstar</td>\n",
       "      <td>Didclab-ws10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_size  number_of_files  bandwidth   rtt  buffer_size  p  cc  pp  fast  \\\n",
       "0     256036             1000        128  0.25      4194304  1   1   1     1   \n",
       "1     256036             1000        128  0.25      4194304  1   1   1     0   \n",
       "2     256036             1000        128  0.25      4194304  1   1   2     1   \n",
       "3     256036             1000        128  0.25      4194304  1   1   2     0   \n",
       "4     256036             1000        128  0.25      4194304  1   1   4     1   \n",
       "\n",
       "   throughput      time          start_time    source   destination  \n",
       "0   50.498704  4.835340 2014-11-08 00:38:03  Evenstar  Didclab-ws10  \n",
       "1   36.190913  6.746920 2014-11-08 00:38:08  Evenstar  Didclab-ws10  \n",
       "2   42.996127  5.679057 2014-11-08 00:38:14  Evenstar  Didclab-ws10  \n",
       "3   41.912495  5.825888 2014-11-08 00:38:20  Evenstar  Didclab-ws10  \n",
       "4   47.204078  5.172809 2014-11-08 00:38:26  Evenstar  Didclab-ws10  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_kemal_local_' + current_time_str\n",
    "\n",
    "# remove milliseconds from the time:\n",
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%Y-%m-%d:%H:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "header_flag = True\n",
    "for file_name in file_names:\n",
    "    full_file_name = file_dir + file_name\n",
    "    \n",
    "    # read the csv file : \n",
    "    data = pds.read_csv(full_file_name)\n",
    "    \n",
    "    # remove wierd last column :\n",
    "    del data['Unnamed: 15']\n",
    "    \n",
    "    # Merge the date and time column: \n",
    "    data['start_time'] = data['date'] +':'+ data['stime']\n",
    "    \n",
    "    # delete Time and Date column :\n",
    "    del data['stime']\n",
    "    del data['date']\n",
    "    \n",
    "    # reordering the columns inside the dataframe:\n",
    "    column_order = ['file_size', 'number_of_files', 'bandwidth', 'rtt', 'buffer_size', \n",
    "                'p', 'cc', 'pp', 'fast', 'throughput', 'time', 'start_time', 'source', 'destination']\n",
    "    data = data[column_order]\n",
    "    \n",
    "    # remove millisecond from the time : \n",
    "    temp = data.start_time.str.split('.')\n",
    "    temp = temp.apply(remove_miliseconds)\n",
    "    data['start_time']=temp\n",
    "    \n",
    "    \n",
    "    # change the current string date time to Timestamp: \n",
    "    data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "    \n",
    "    \n",
    "    # Unit conversion:\n",
    "    # file_size - MegaByte\n",
    "    # bandwidth - MegaByte per second\n",
    "    # buffer_size - MegaByte\n",
    "    # rtt - millisecond\n",
    "    # throughput - MegaByte per second\n",
    "    # time - second \n",
    "    # start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "    #d = data.copy(deep=True)\n",
    "    #data['file_size'  = (data['file_size'] )\n",
    "    data['bandwidth'] = int(128)\n",
    "    #data['buffer_size'] = (data['buffer_size'] )\n",
    "    data['rtt'] = '0.25'\n",
    "    data['throughput'] = data['throughput'] / 8 \n",
    "    \n",
    "    # filtering data based on throughput : achievable throughput has shown less then 90% of the bandwidth.\n",
    "    th_filter = data.throughput <  ( data.bandwidth * 0.90)\n",
    "    data = data[th_filter]\n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the unique transfers : as tuple \n",
    "d = data[['file_size','number_of_files','bandwidth','rtt','buffer_size']]\n",
    "d = d.drop_duplicates()\n",
    "d['file_size'] = d['file_size'].round(decimals=2)\n",
    "unique_transfers = [tuple(transfers) for transfers in d.values]\n",
    "unique_transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make (file_size, number_of_files, bandwidth, rtt, buffer_size) index \n",
    "d = data.set_index(['file_size','number_of_files','bandwidth','rtt','buffer_size','p','cc','pp','fast'])\n",
    "\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look-up the data:\n",
    "t = d.loc[unique_transfers[0]].throughput\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for engin's data : old :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EDData.csv', 'BTData.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the file list : \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/engin/old/'\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>number_of_files</th>\n",
       "      <th>bandwidth</th>\n",
       "      <th>rtt</th>\n",
       "      <th>buffer_size</th>\n",
       "      <th>p</th>\n",
       "      <th>cc</th>\n",
       "      <th>pp</th>\n",
       "      <th>fast</th>\n",
       "      <th>throughput</th>\n",
       "      <th>time</th>\n",
       "      <th>start_time</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3131997</td>\n",
       "      <td>3429</td>\n",
       "      <td>1280</td>\n",
       "      <td>80</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25.187269</td>\n",
       "      <td>406.637939</td>\n",
       "      <td>2015-03-31 00:57:22</td>\n",
       "      <td>Blacklight</td>\n",
       "      <td>Trestles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3131997</td>\n",
       "      <td>3429</td>\n",
       "      <td>1280</td>\n",
       "      <td>80</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>35.116388</td>\n",
       "      <td>291.661520</td>\n",
       "      <td>2015-03-31 00:04:09</td>\n",
       "      <td>Blacklight</td>\n",
       "      <td>Trestles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3131997</td>\n",
       "      <td>3429</td>\n",
       "      <td>1280</td>\n",
       "      <td>80</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23.076415</td>\n",
       "      <td>443.834042</td>\n",
       "      <td>2015-03-31 00:09:01</td>\n",
       "      <td>Blacklight</td>\n",
       "      <td>Trestles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3131997</td>\n",
       "      <td>3429</td>\n",
       "      <td>1280</td>\n",
       "      <td>80</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>32.948817</td>\n",
       "      <td>310.848773</td>\n",
       "      <td>2015-03-31 00:16:25</td>\n",
       "      <td>Blacklight</td>\n",
       "      <td>Trestles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3131997</td>\n",
       "      <td>3429</td>\n",
       "      <td>1280</td>\n",
       "      <td>80</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>42.757196</td>\n",
       "      <td>239.540963</td>\n",
       "      <td>2015-03-31 00:21:35</td>\n",
       "      <td>Blacklight</td>\n",
       "      <td>Trestles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_size  number_of_files  bandwidth  rtt  buffer_size  p  cc  pp  fast  \\\n",
       "0    3131997             3429       1280   80     33554432  1   1   1     0   \n",
       "1    3131997             3429       1280   80     33554432  1   1   2     0   \n",
       "2    3131997             3429       1280   80     33554432  1   1   4     0   \n",
       "3    3131997             3429       1280   80     33554432  1   1   8     0   \n",
       "4    3131997             3429       1280   80     33554432  1   1  16     0   \n",
       "\n",
       "   throughput        time          start_time      source destination  \n",
       "0   25.187269  406.637939 2015-03-31 00:57:22  Blacklight    Trestles  \n",
       "1   35.116388  291.661520 2015-03-31 00:04:09  Blacklight    Trestles  \n",
       "2   23.076415  443.834042 2015-03-31 00:09:01  Blacklight    Trestles  \n",
       "3   32.948817  310.848773 2015-03-31 00:16:25  Blacklight    Trestles  \n",
       "4   42.757196  239.540963 2015-03-31 00:21:35  Blacklight    Trestles  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_engin_old_' + current_time_str\n",
    "\n",
    "# remove milliseconds from the time:\n",
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%m/%d/%y:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "header_flag = True\n",
    "bandwidth = int(128)   # MegaByte per second\n",
    "rtt = '0.25'\n",
    "for file_name in file_names:\n",
    "    full_file_name = file_dir + file_name\n",
    "    \n",
    "    # read the csv file : \n",
    "    data = pds.read_csv(full_file_name)\n",
    "\n",
    "    # merge the date and time column:\n",
    "    data['start_time'] = data['date'] +':'+ data['stime']\n",
    "    \n",
    "    # delete both Date and Time column:\n",
    "    del data['stime']\n",
    "    del data['date']\n",
    "    \n",
    "    # reordering the columns in data frame:\n",
    "    column_order = ['file_size', 'number_of_files', 'bandwidth', 'rtt', 'buffer_size', \n",
    "                'p', 'cc', 'pp', 'fast', 'throughput', 'time', 'start_time', 'source', 'destination']\n",
    "    data = data[column_order]\n",
    "\n",
    "    # remove millisecond part from the data:\n",
    "    temp = data.start_time.str.split('.')\n",
    "    temp = temp.apply(remove_miliseconds)\n",
    "    data['start_time']=temp\n",
    "    data.head()\n",
    "    \n",
    "    # change the current string date time to Timestamp: \n",
    "    data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "    \n",
    "\n",
    "    # Unit conversion:\n",
    "    # file_size - MegaByte\n",
    "    # bandwidth - MegaByte per second\n",
    "    # buffer_size - MegaByte\n",
    "    # rtt - millisecond\n",
    "    # throughput - MegaByte per second\n",
    "    # time - second \n",
    "    # start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "    #data = data.copy(deep=True)\n",
    "    #data['file_size'] = (data['file_size']) \n",
    "    data['bandwidth'] = bandwidth\n",
    "    #data['buffer_size'] = (data['buffer_size'] )\n",
    "    data['rtt'] = rtt\n",
    "    data['throughput'] = data['throughput'] / 8 \n",
    "    \n",
    "    # filtering data based on throughput : achievable throughput has shown less then 90% of the bandwidth.\n",
    "    th_filter = data.throughput <  ( data.bandwidth * 0.90)\n",
    "    data = data[th_filter]\n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "    bandwidth = int(1280)\n",
    "    rtt = int(80)\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for engin's data : new :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SGData.csv', 'SBData.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the file list : \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/engin/new/'\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>number_of_files</th>\n",
       "      <th>bandwidth</th>\n",
       "      <th>rtt</th>\n",
       "      <th>buffer_size</th>\n",
       "      <th>p</th>\n",
       "      <th>cc</th>\n",
       "      <th>pp</th>\n",
       "      <th>fast</th>\n",
       "      <th>throughput</th>\n",
       "      <th>time</th>\n",
       "      <th>start_time</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3136594</td>\n",
       "      <td>3424</td>\n",
       "      <td>1280</td>\n",
       "      <td>40</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.371090</td>\n",
       "      <td>191.904988</td>\n",
       "      <td>2015-04-10 11:11:55</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Blacklight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3136594</td>\n",
       "      <td>3424</td>\n",
       "      <td>1280</td>\n",
       "      <td>40</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>57.902133</td>\n",
       "      <td>176.887784</td>\n",
       "      <td>2015-04-10 11:15:07</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Blacklight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3136594</td>\n",
       "      <td>3424</td>\n",
       "      <td>1280</td>\n",
       "      <td>40</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>71.170525</td>\n",
       "      <td>143.910400</td>\n",
       "      <td>2015-04-10 11:18:04</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Blacklight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3136594</td>\n",
       "      <td>3424</td>\n",
       "      <td>1280</td>\n",
       "      <td>40</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>60.435848</td>\n",
       "      <td>169.471903</td>\n",
       "      <td>2015-04-10 11:20:27</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Blacklight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3136594</td>\n",
       "      <td>3424</td>\n",
       "      <td>1280</td>\n",
       "      <td>40</td>\n",
       "      <td>33554432</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>50.593947</td>\n",
       "      <td>202.438798</td>\n",
       "      <td>2015-04-10 11:23:17</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Blacklight</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_size  number_of_files  bandwidth rtt  buffer_size  p  cc  pp  fast  \\\n",
       "0    3136594             3424       1280  40     33554432  1   1   1     0   \n",
       "1    3136594             3424       1280  40     33554432  1   1   2     0   \n",
       "2    3136594             3424       1280  40     33554432  1   1   4     0   \n",
       "3    3136594             3424       1280  40     33554432  1   1   8     0   \n",
       "4    3136594             3424       1280  40     33554432  1   1  16     0   \n",
       "\n",
       "   throughput        time          start_time    source destination  \n",
       "0   53.371090  191.904988 2015-04-10 11:11:55  Stampede  Blacklight  \n",
       "1   57.902133  176.887784 2015-04-10 11:15:07  Stampede  Blacklight  \n",
       "2   71.170525  143.910400 2015-04-10 11:18:04  Stampede  Blacklight  \n",
       "3   60.435848  169.471903 2015-04-10 11:20:27  Stampede  Blacklight  \n",
       "4   50.593947  202.438798 2015-04-10 11:23:17  Stampede  Blacklight  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_engin_new_' + current_time_str\n",
    "\n",
    "# remove milliseconds from the time:\n",
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%Y-%m-%d:%H:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "header_flag = True\n",
    "for file_name in file_names:\n",
    "    full_file_name = file_dir + file_name\n",
    "    \n",
    "    # read the csv file : \n",
    "    data = pds.read_csv(full_file_name)\n",
    "\n",
    "    # merge the date and time column:\n",
    "    data['start_time'] = data['date'] +':'+ data['stime']\n",
    "    \n",
    "    # delete both Date and Time column:\n",
    "    del data['stime']\n",
    "    del data['date']\n",
    "    \n",
    "    # reordering the columns in data frame:\n",
    "    column_order = ['file_size', 'number_of_files', 'bandwidth', 'rtt', 'buffer_size', \n",
    "                'p', 'cc', 'pp', 'fast', 'throughput', 'time', 'start_time', 'source', 'destination']\n",
    "    data = data[column_order]\n",
    "\n",
    "    # remove millisecond part from the data:\n",
    "    temp = data.start_time.str.split('.')\n",
    "    temp = temp.apply(remove_miliseconds)\n",
    "    data['start_time']=temp\n",
    "    \n",
    "    \n",
    "    # change the current string date time to Timestamp: \n",
    "    data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "    \n",
    "\n",
    "    # Unit conversion:\n",
    "    # file_size - MegaByte\n",
    "    # bandwidth - MegaByte per second\n",
    "    # buffer_size - MegaByte\n",
    "    # rtt - millisecond\n",
    "    # throughput - MegaByte per second\n",
    "    # time - second \n",
    "    # start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "    #data = data.copy(deep=True)\n",
    "    #data['file_size'] = (data['file_size'] / (1024 * 1024) ).round(decimals=2)\n",
    "    data['bandwidth'] = int(1280)\n",
    "    #data['buffer_size'] = (data['buffer_size'] / (1024 * 1024) ).round(decimals=2)\n",
    "    data['rtt'] = '40'\n",
    "    data['throughput'] = data['throughput'] / 8 \n",
    "    \n",
    "    # filtering data based on throughput : achievable throughput has shown less then 90% of the bandwidth.\n",
    "    th_filter = data.throughput <  ( data.bandwidth * 0.90)\n",
    "    data = data[th_filter]\n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "\n",
    "data.head()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.throughput.sort_values().tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading my(Zulkar) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log_stampede-scratch_gordon-scratch_date06_02_2016_18_58_size_small.txt',\n",
       " 'log_stampede-scratch_gordon-scratch_date05_20_2016_11_23.txt',\n",
       " 'log_stampede-scratch_gordon-scratch_date05_19_2016_08_38.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the file list : \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#linux directory:\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/zulkar_6_2016/'\n",
    "# mac directory: \n",
    "\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>number_of_files</th>\n",
       "      <th>bandwidth</th>\n",
       "      <th>rtt</th>\n",
       "      <th>buffer_size</th>\n",
       "      <th>p</th>\n",
       "      <th>cc</th>\n",
       "      <th>pp</th>\n",
       "      <th>fast</th>\n",
       "      <th>throughput</th>\n",
       "      <th>time</th>\n",
       "      <th>start_time</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>209715259</td>\n",
       "      <td>69</td>\n",
       "      <td>1280</td>\n",
       "      <td>39</td>\n",
       "      <td>49920000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>221.529945</td>\n",
       "      <td>62.294079</td>\n",
       "      <td>2016-05-19 08:39:00</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Gordon/Oasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>209715259</td>\n",
       "      <td>69</td>\n",
       "      <td>1280</td>\n",
       "      <td>39</td>\n",
       "      <td>49920000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>244.455999</td>\n",
       "      <td>56.451893</td>\n",
       "      <td>2016-05-19 08:40:00</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Gordon/Oasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>209715259</td>\n",
       "      <td>69</td>\n",
       "      <td>1280</td>\n",
       "      <td>39</td>\n",
       "      <td>49920000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>246.596702</td>\n",
       "      <td>55.961835</td>\n",
       "      <td>2016-05-19 08:41:00</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Gordon/Oasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>209715259</td>\n",
       "      <td>69</td>\n",
       "      <td>1280</td>\n",
       "      <td>39</td>\n",
       "      <td>49920000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>238.490156</td>\n",
       "      <td>57.864040</td>\n",
       "      <td>2016-05-19 08:42:00</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Gordon/Oasis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>209715259</td>\n",
       "      <td>69</td>\n",
       "      <td>1280</td>\n",
       "      <td>39</td>\n",
       "      <td>49920000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>235.796879</td>\n",
       "      <td>58.524964</td>\n",
       "      <td>2016-05-19 08:43:00</td>\n",
       "      <td>Stampede</td>\n",
       "      <td>Gordon/Oasis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_size  number_of_files  bandwidth  rtt  buffer_size  p  cc  pp  fast  \\\n",
       "0  209715259               69       1280   39     49920000  1   1   1     0   \n",
       "1  209715259               69       1280   39     49920000  1   1   1     1   \n",
       "2  209715259               69       1280   39     49920000  1   1   2     0   \n",
       "3  209715259               69       1280   39     49920000  1   1   2     1   \n",
       "4  209715259               69       1280   39     49920000  1   1   4     0   \n",
       "\n",
       "   throughput       time          start_time    source   destination  \n",
       "0  221.529945  62.294079 2016-05-19 08:39:00  Stampede  Gordon/Oasis  \n",
       "1  244.455999  56.451893 2016-05-19 08:40:00  Stampede  Gordon/Oasis  \n",
       "2  246.596702  55.961835 2016-05-19 08:41:00  Stampede  Gordon/Oasis  \n",
       "3  238.490156  57.864040 2016-05-19 08:42:00  Stampede  Gordon/Oasis  \n",
       "4  235.796879  58.524964 2016-05-19 08:43:00  Stampede  Gordon/Oasis  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_zulkar_6_16_' + current_time_str\n",
    "\n",
    "# remove milliseconds from the time:\n",
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%Y-%m-%d:%H:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "\n",
    "header_flag = True\n",
    "for file_name in file_names:\n",
    "    full_file_name = file_dir + file_name\n",
    "    \n",
    "    # read the csv file : \n",
    "    seperator = '\\t'\n",
    "    data = pds.read_table(full_file_name,sep=seperator)\n",
    "    \n",
    "    # add source and destination in each data row:\n",
    "    data['source'] = 'Stampede'\n",
    "    data['destination'] = 'Gordon/Oasis'\n",
    "\n",
    "    \n",
    "    # convert the string timestamp in dataType: Timestamp\n",
    "    format_date_time = '%m:%d:%Y-%H:%M'\n",
    "    #format_date_time = '%m:%d:%Y-%H:%M'\n",
    "    def convert_to_time_stamp(date_in_string):\n",
    "        result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "        return result\n",
    "\n",
    "    # change the current string date time to Timestamp: \n",
    "    data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "    \n",
    "    # Unit conversion:\n",
    "    # file_size - MegaByte\n",
    "    # bandwidth - MegaByte per second\n",
    "    # buffer_size - MegaByte\n",
    "    # rtt - millisecond\n",
    "    # throughput - MegaByte per second\n",
    "    # time - second \n",
    "    # start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "    #data = data.copy(deep=True)\n",
    "    #data['file_size'] = (data['file_size'] / (1024 * 1024) ).round(decimals=2)\n",
    "    data['bandwidth'] = int(1280)\n",
    "    #data['buffer_size'] = (data['buffer_size'] / (1024 * 1024) ).round(decimals=2)\n",
    "    #d['rtt'] = (d['rtt'] * 1000 )\n",
    "    #d['throughput'] = d['throughput'] / 8 \n",
    "    \n",
    "    \n",
    "    # filtering data based on throughput : achievable throughput has shown less then 90% of the bandwidth.\n",
    "    th_filter = data.throughput <  ( data.bandwidth * 0.90)\n",
    "    data = data[th_filter]\n",
    "    \n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "    \n",
    "    \n",
    "data.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_' + current_time_str\n",
    "\n",
    "# read files: \n",
    "# Get the file list : \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#linux directory:\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/sub_master/'\n",
    "# mac directory: \n",
    "\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names\n",
    "\n",
    "header_flag = True\n",
    "for file_name in file_names:\n",
    "    \n",
    "    # read the mini-master files:\n",
    "    full_file_name = file_dir + file_name\n",
    "    data = pds.read_csv(full_file_name)\n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merger additional datasets to master.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# provide the current master file:\n",
    "input_master_file = ''\n",
    "\n",
    "\n",
    "#linux directory:\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/master/'\n",
    "# mac directory: \n",
    "\n",
    "\n",
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_' + current_time_str\n",
    "\n",
    "# to do : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file might not come with header information. In such case add the following line with appropriate seperator:\n",
    "# file_Size_bytes\tnumber_of_files\tbandwidth_Gbps\trtt_ms\tbuffer_size_bytes\tp\tcc\tpp\tfast\tthroughput_MByteps\ttime_sec\tstart_time_mm_dd_yyyy_hh_mm\n",
    "# read data from a csv file without reading the index : \n",
    "dd = pds.read_csv('')\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Templete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file might not come with header information. In such case add the following line with appropriate seperator:\n",
    "# file_Size_bytes\tnumber_of_files\tbandwidth_Gbps\trtt_ms\tbuffer_size_bytes\tp\tcc\tpp\tfast\tthroughput_MByteps\ttime_sec\tstart_time_mm_dd_yyyy_hh_mm\n",
    "\n",
    "full_file_name = file_dir + file_names[0]\n",
    "data = pds.read_csv(full_file_name,index_col=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "del data['Unnamed: 15']\n",
    "data['start_time'] = data['date'] +':'+ data['stime']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del data['stime']\n",
    "del data['date']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_order = ['file_size', 'number_of_files', 'bandwidth', 'rtt', 'buffer_size', \n",
    "                'p', 'cc', 'pp', 'fast', 'throughput', 'time', 'start_time', 'source', 'destination']\n",
    "data = data[column_order]\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "temp = data.start_time.str.split('.')\n",
    "temp = temp.apply(remove_miliseconds)\n",
    "data['start_time']=temp\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%Y-%m-%d:%H:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "# change the current string date time to Timestamp: \n",
    "data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unit conversion:\n",
    "# file_size - MegaByte\n",
    "# bandwidth - MegaByte per second\n",
    "# buffer_size - MegaByte\n",
    "# rtt - millisecond\n",
    "# throughput - MegaByte per second\n",
    "# time - second \n",
    "# start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "#d = data.copy(deep=True)\n",
    "data['file_size'] = (data['file_size'] / (1000 * 1000) ).round(decimals=2)\n",
    "data['bandwidth'] = data['bandwidth'] / 8\n",
    "data['buffer_size'] = (data['buffer_size'] / (1000 * 1000) ).round(decimals=2)\n",
    "data['rtt'] = (data['rtt'] * 1000 )\n",
    "data['throughput'] = data['throughput'] / 8 \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# write data to csv file:\n",
    "data.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data from a csv file without reading the index : \n",
    "dd = pds.read_csv('my_csv.csv')\n",
    "dd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# append data in an existing csv file with header false :\n",
    "with open('my_csv.csv', 'a') as f:\n",
    "    df.to_csv(f, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#d = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
