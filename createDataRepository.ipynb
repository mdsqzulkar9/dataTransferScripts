{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This script will read all the data from the data/ThOpt/ folder\n",
    "Then it will create an uniform csv file containing all the data.\n",
    "\n",
    "Each segment will create some pseudo master files.\n",
    "(e.g. master_engin_new_06_09_2016_12_33  \n",
    "master_engin_old_06_09_2016_12_33  \n",
    "master_kemal_06_09_2016_12_30  \n",
    "master_zulkar_6_16_06_09_2016_12_33)\n",
    "\n",
    "all those files are marged to make \"master_<datetime>.csv\" file.\n",
    "\n",
    "The output will be a \"master_<datetime>.csv\"\n",
    "Columns in 'master.csv' file are :\n",
    "file_size - Bytes (int)\n",
    "number_of_files - counts\n",
    "bandwidth - MegaByte (int)\n",
    "rtt - millisecond (int)\n",
    "buffer_size - Bytes \n",
    "p - count (bound <= 32)\n",
    "cc - count (bound <= 32)\n",
    "pp - count (bound <= 32)\n",
    "fast - boolean {0,1}\n",
    "throughput - MegaByte Per Second\n",
    "time - Seconds\n",
    "start_time - yyyy-mm-dd HH:MM:SS (e.g. 2015-03-31 00:57:22)\n",
    "source - string name\n",
    "destination - string name\n",
    "\n",
    "always open it in append mode if you want to add extra data in it. \n",
    "\n",
    "Author: Md S Q Zulkar Nine   Start Date : June 8, 2016    Last Modified: June 10, 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas library:\n",
    "import pandas as pds\n",
    "import pylab as pyl\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pretty dataframe :\n",
    "from IPython.core.display import HTML\n",
    "css = open('style-table.css').read() + open('style-notebook.css').read()\n",
    "HTML('<style>{}</style>'.format(css))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the file list : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/kemal/xsede/'\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for Kemal's data: xsede - xsede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_kemal_xsede_' + current_time_str\n",
    "\n",
    "# remove milliseconds from the time:\n",
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%Y-%m-%d:%H:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "header_flag = True\n",
    "for file_name in file_names:\n",
    "    full_file_name = file_dir + file_name\n",
    "    \n",
    "    # read the csv file : \n",
    "    data = pds.read_csv(full_file_name)\n",
    "    \n",
    "    # remove wierd last column :\n",
    "    del data['Unnamed: 15']\n",
    "    \n",
    "    # Merge the date and time column: \n",
    "    data['start_time'] = data['date'] +':'+ data['stime']\n",
    "    \n",
    "    # delete Time and Date column :\n",
    "    del data['stime']\n",
    "    del data['date']\n",
    "    \n",
    "    # reordering the columns inside the dataframe:\n",
    "    column_order = ['file_size', 'number_of_files', 'bandwidth', 'rtt', 'buffer_size', \n",
    "                'p', 'cc', 'pp', 'fast', 'throughput', 'time', 'start_time', 'source', 'destination']\n",
    "    data = data[column_order]\n",
    "    \n",
    "    # remove millisecond from the time : \n",
    "    temp = data.start_time.str.split('.')\n",
    "    temp = temp.apply(remove_miliseconds)\n",
    "    data['start_time']=temp\n",
    "    \n",
    "    \n",
    "    # change the current string date time to Timestamp: \n",
    "    data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "    \n",
    "    \n",
    "    # Unit conversion:\n",
    "    # file_size - Byte\n",
    "    # bandwidth - MegaByte per second\n",
    "    # buffer_size - Byte\n",
    "    # rtt - millisecond\n",
    "    # throughput - MegaByte per second\n",
    "    # time - second \n",
    "    # start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "    #d = data.copy(deep=True)\n",
    "    data['file_size'] = (data['file_size'])\n",
    "    data['bandwidth'] = int(1280)\n",
    "    data['buffer_size'] = (data['buffer_size'] )\n",
    "    data['rtt'] = '40'\n",
    "    data['throughput'] = data['throughput'] / 8 \n",
    "    \n",
    "    # filtering data based on throughput : achievable throughput has shown less then 90% of the bandwidth.\n",
    "    th_filter = data.throughput <  ( data.bandwidth * 0.90)\n",
    "    data = data[th_filter]\n",
    "    \n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for Kemal's data: Local to Local: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/kemal/local/'\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_kemal_local_' + current_time_str\n",
    "\n",
    "# remove milliseconds from the time:\n",
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%Y-%m-%d:%H:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "header_flag = True\n",
    "for file_name in file_names:\n",
    "    full_file_name = file_dir + file_name\n",
    "    \n",
    "    # read the csv file : \n",
    "    data = pds.read_csv(full_file_name)\n",
    "    \n",
    "    # remove wierd last column :\n",
    "    del data['Unnamed: 15']\n",
    "    \n",
    "    # Merge the date and time column: \n",
    "    data['start_time'] = data['date'] +':'+ data['stime']\n",
    "    \n",
    "    # delete Time and Date column :\n",
    "    del data['stime']\n",
    "    del data['date']\n",
    "    \n",
    "    # reordering the columns inside the dataframe:\n",
    "    column_order = ['file_size', 'number_of_files', 'bandwidth', 'rtt', 'buffer_size', \n",
    "                'p', 'cc', 'pp', 'fast', 'throughput', 'time', 'start_time', 'source', 'destination']\n",
    "    data = data[column_order]\n",
    "    \n",
    "    # remove millisecond from the time : \n",
    "    temp = data.start_time.str.split('.')\n",
    "    temp = temp.apply(remove_miliseconds)\n",
    "    data['start_time']=temp\n",
    "    \n",
    "    \n",
    "    # change the current string date time to Timestamp: \n",
    "    data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "    \n",
    "    \n",
    "    # Unit conversion:\n",
    "    # file_size - MegaByte\n",
    "    # bandwidth - MegaByte per second\n",
    "    # buffer_size - MegaByte\n",
    "    # rtt - millisecond\n",
    "    # throughput - MegaByte per second\n",
    "    # time - second \n",
    "    # start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "    #d = data.copy(deep=True)\n",
    "    #data['file_size'  = (data['file_size'] )\n",
    "    data['bandwidth'] = int(128)\n",
    "    #data['buffer_size'] = (data['buffer_size'] )\n",
    "    data['rtt'] = '0.25'\n",
    "    data['throughput'] = data['throughput'] / 8 \n",
    "    \n",
    "    # filtering data based on throughput : achievable throughput has shown less then 90% of the bandwidth.\n",
    "    th_filter = data.throughput <  ( data.bandwidth * 0.90)\n",
    "    data = data[th_filter]\n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "data.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the unique transfers : as tuple \n",
    "d = data[['file_size','number_of_files','bandwidth','rtt','buffer_size']]\n",
    "d = d.drop_duplicates()\n",
    "d['file_size'] = d['file_size'].round(decimals=2)\n",
    "unique_transfers = [tuple(transfers) for transfers in d.values]\n",
    "unique_transfers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make (file_size, number_of_files, bandwidth, rtt, buffer_size) index \n",
    "d = data.set_index(['file_size','number_of_files','bandwidth','rtt','buffer_size','p','cc','pp','fast'])\n",
    "\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# look-up the data:\n",
    "t = d.loc[unique_transfers[0]].throughput\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for engin's data : old :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the file list : \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/engin/old/'\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_engin_old_' + current_time_str\n",
    "\n",
    "# remove milliseconds from the time:\n",
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%m/%d/%y:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "header_flag = True\n",
    "bandwidth = int(128)   # MegaByte per second\n",
    "rtt = '0.25'\n",
    "for file_name in file_names:\n",
    "    full_file_name = file_dir + file_name\n",
    "    \n",
    "    # read the csv file : \n",
    "    data = pds.read_csv(full_file_name)\n",
    "\n",
    "    # merge the date and time column:\n",
    "    data['start_time'] = data['date'] +':'+ data['stime']\n",
    "    \n",
    "    # delete both Date and Time column:\n",
    "    del data['stime']\n",
    "    del data['date']\n",
    "    \n",
    "    # reordering the columns in data frame:\n",
    "    column_order = ['file_size', 'number_of_files', 'bandwidth', 'rtt', 'buffer_size', \n",
    "                'p', 'cc', 'pp', 'fast', 'throughput', 'time', 'start_time', 'source', 'destination']\n",
    "    data = data[column_order]\n",
    "\n",
    "    # remove millisecond part from the data:\n",
    "    temp = data.start_time.str.split('.')\n",
    "    temp = temp.apply(remove_miliseconds)\n",
    "    data['start_time']=temp\n",
    "    data.head()\n",
    "    \n",
    "    # change the current string date time to Timestamp: \n",
    "    data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "    \n",
    "\n",
    "    # Unit conversion:\n",
    "    # file_size - MegaByte\n",
    "    # bandwidth - MegaByte per second\n",
    "    # buffer_size - MegaByte\n",
    "    # rtt - millisecond\n",
    "    # throughput - MegaByte per second\n",
    "    # time - second \n",
    "    # start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "    #data = data.copy(deep=True)\n",
    "    #data['file_size'] = (data['file_size']) \n",
    "    data['bandwidth'] = bandwidth\n",
    "    #data['buffer_size'] = (data['buffer_size'] )\n",
    "    data['rtt'] = rtt\n",
    "    data['throughput'] = data['throughput'] / 8 \n",
    "    \n",
    "    # filtering data based on throughput : achievable throughput has shown less then 90% of the bandwidth.\n",
    "    th_filter = data.throughput <  ( data.bandwidth * 0.90)\n",
    "    data = data[th_filter]\n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "    bandwidth = int(1280)\n",
    "    rtt = int(80)\n",
    "    \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is for engin's data : new :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the file list : \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/engin/new/'\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_engin_new_' + current_time_str\n",
    "\n",
    "# remove milliseconds from the time:\n",
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%Y-%m-%d:%H:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "header_flag = True\n",
    "for file_name in file_names:\n",
    "    full_file_name = file_dir + file_name\n",
    "    \n",
    "    # read the csv file : \n",
    "    data = pds.read_csv(full_file_name)\n",
    "\n",
    "    # merge the date and time column:\n",
    "    data['start_time'] = data['date'] +':'+ data['stime']\n",
    "    \n",
    "    # delete both Date and Time column:\n",
    "    del data['stime']\n",
    "    del data['date']\n",
    "    \n",
    "    # reordering the columns in data frame:\n",
    "    column_order = ['file_size', 'number_of_files', 'bandwidth', 'rtt', 'buffer_size', \n",
    "                'p', 'cc', 'pp', 'fast', 'throughput', 'time', 'start_time', 'source', 'destination']\n",
    "    data = data[column_order]\n",
    "\n",
    "    # remove millisecond part from the data:\n",
    "    temp = data.start_time.str.split('.')\n",
    "    temp = temp.apply(remove_miliseconds)\n",
    "    data['start_time']=temp\n",
    "    \n",
    "    \n",
    "    # change the current string date time to Timestamp: \n",
    "    data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "    \n",
    "\n",
    "    # Unit conversion:\n",
    "    # file_size - MegaByte\n",
    "    # bandwidth - MegaByte per second\n",
    "    # buffer_size - MegaByte\n",
    "    # rtt - millisecond\n",
    "    # throughput - MegaByte per second\n",
    "    # time - second \n",
    "    # start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "    #data = data.copy(deep=True)\n",
    "    #data['file_size'] = (data['file_size'] / (1024 * 1024) ).round(decimals=2)\n",
    "    data['bandwidth'] = int(1280)\n",
    "    #data['buffer_size'] = (data['buffer_size'] / (1024 * 1024) ).round(decimals=2)\n",
    "    data['rtt'] = '40'\n",
    "    data['throughput'] = data['throughput'] / 8 \n",
    "    \n",
    "    # filtering data based on throughput : achievable throughput has shown less then 90% of the bandwidth.\n",
    "    th_filter = data.throughput <  ( data.bandwidth * 0.90)\n",
    "    data = data[th_filter]\n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "\n",
    "data.head()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.throughput.sort_values().tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading my(Zulkar) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the file list : \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#linux directory:\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/zulkar_6_2016/'\n",
    "# mac directory: \n",
    "\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_zulkar_6_16_' + current_time_str\n",
    "\n",
    "# remove milliseconds from the time:\n",
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%Y-%m-%d:%H:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "\n",
    "header_flag = True\n",
    "for file_name in file_names:\n",
    "    full_file_name = file_dir + file_name\n",
    "    \n",
    "    # read the csv file : \n",
    "    seperator = '\\t'\n",
    "    data = pds.read_table(full_file_name,sep=seperator)\n",
    "    \n",
    "    # add source and destination in each data row:\n",
    "    data['source'] = 'Stampede'\n",
    "    data['destination'] = 'Gordon/Oasis'\n",
    "\n",
    "    \n",
    "    # convert the string timestamp in dataType: Timestamp\n",
    "    format_date_time = '%m:%d:%Y-%H:%M'\n",
    "    #format_date_time = '%m:%d:%Y-%H:%M'\n",
    "    def convert_to_time_stamp(date_in_string):\n",
    "        result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "        return result\n",
    "\n",
    "    # change the current string date time to Timestamp: \n",
    "    data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "    \n",
    "    # Unit conversion:\n",
    "    # file_size - MegaByte\n",
    "    # bandwidth - MegaByte per second\n",
    "    # buffer_size - MegaByte\n",
    "    # rtt - millisecond\n",
    "    # throughput - MegaByte per second\n",
    "    # time - second \n",
    "    # start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "    #data = data.copy(deep=True)\n",
    "    #data['file_size'] = (data['file_size'] / (1024 * 1024) ).round(decimals=2)\n",
    "    data['bandwidth'] = int(1280)\n",
    "    #data['buffer_size'] = (data['buffer_size'] / (1024 * 1024) ).round(decimals=2)\n",
    "    #d['rtt'] = (d['rtt'] * 1000 )\n",
    "    #d['throughput'] = d['throughput'] / 8 \n",
    "    \n",
    "    \n",
    "    # filtering data based on throughput : achievable throughput has shown less then 90% of the bandwidth.\n",
    "    th_filter = data.throughput <  ( data.bandwidth * 0.90)\n",
    "    data = data[th_filter]\n",
    "    \n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "    \n",
    "    \n",
    "data.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_' + current_time_str\n",
    "\n",
    "# read files: \n",
    "# Get the file list : \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "#linux directory:\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/sub_master/'\n",
    "# mac directory: \n",
    "\n",
    "temp_file_names = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "\n",
    "file_names = []\n",
    "for file_name in temp_file_names:\n",
    "    if '~' not in file_name:\n",
    "        file_names.append(file_name)\n",
    "file_names\n",
    "\n",
    "header_flag = True\n",
    "for file_name in file_names:\n",
    "    \n",
    "    # read the mini-master files:\n",
    "    full_file_name = file_dir + file_name\n",
    "    data = pds.read_csv(full_file_name)\n",
    "    \n",
    "    # append data in an existing csv file with header false :\n",
    "    with open(output_file, 'a') as f:\n",
    "        data.to_csv(f, header=header_flag,index=False)\n",
    "\n",
    "    header_flag = False\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# load data set and add some minor patch work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "master_06_10_2016_18_27  master_06_29_2016_14_52  readme.txt\r\n"
     ]
    }
   ],
   "source": [
    "# See the dataset folder in linux-lab:\n",
    "! ls /home/zulkar/Dropbox/gits/data/ThOpt/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Control variables: \n",
    "# master dataset location:\n",
    "master_location = 'Dropbox/gits/data/ThOpt/master/'\n",
    "# master file name : \n",
    "master_file_name = 'master_06_10_2016_18_27'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>number_of_files</th>\n",
       "      <th>bandwidth</th>\n",
       "      <th>rtt</th>\n",
       "      <th>buffer_size</th>\n",
       "      <th>p</th>\n",
       "      <th>cc</th>\n",
       "      <th>pp</th>\n",
       "      <th>fast</th>\n",
       "      <th>throughput</th>\n",
       "      <th>time</th>\n",
       "      <th>start_time</th>\n",
       "      <th>source</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>652486</td>\n",
       "      <td>3293</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4194304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>48.360560</td>\n",
       "      <td>42.371396</td>\n",
       "      <td>2015-07-24 00:05:45</td>\n",
       "      <td>Evenstar</td>\n",
       "      <td>Didclab-ws10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>652486</td>\n",
       "      <td>3293</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4194304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>56.777687</td>\n",
       "      <td>36.089951</td>\n",
       "      <td>2015-07-24 00:06:27</td>\n",
       "      <td>Evenstar</td>\n",
       "      <td>Didclab-ws10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>652486</td>\n",
       "      <td>3293</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4194304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>62.801374</td>\n",
       "      <td>32.628335</td>\n",
       "      <td>2015-07-24 00:07:03</td>\n",
       "      <td>Evenstar</td>\n",
       "      <td>Didclab-ws10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>652486</td>\n",
       "      <td>3293</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4194304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>76.870653</td>\n",
       "      <td>26.656531</td>\n",
       "      <td>2015-07-24 00:07:36</td>\n",
       "      <td>Evenstar</td>\n",
       "      <td>Didclab-ws10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>652486</td>\n",
       "      <td>3293</td>\n",
       "      <td>128</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4194304</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>79.185026</td>\n",
       "      <td>25.877430</td>\n",
       "      <td>2015-07-24 00:08:03</td>\n",
       "      <td>Evenstar</td>\n",
       "      <td>Didclab-ws10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_size  number_of_files  bandwidth   rtt  buffer_size  p  cc  pp  fast  \\\n",
       "0     652486             3293        128  0.25      4194304  1   1   1     0   \n",
       "1     652486             3293        128  0.25      4194304  1   1   2     0   \n",
       "2     652486             3293        128  0.25      4194304  1   1   4     0   \n",
       "3     652486             3293        128  0.25      4194304  1   1   8     0   \n",
       "4     652486             3293        128  0.25      4194304  1   1  16     0   \n",
       "\n",
       "   throughput       time           start_time    source   destination  \n",
       "0   48.360560  42.371396  2015-07-24 00:05:45  Evenstar  Didclab-ws10  \n",
       "1   56.777687  36.089951  2015-07-24 00:06:27  Evenstar  Didclab-ws10  \n",
       "2   62.801374  32.628335  2015-07-24 00:07:03  Evenstar  Didclab-ws10  \n",
       "3   76.870653  26.656531  2015-07-24 00:07:36  Evenstar  Didclab-ws10  \n",
       "4   79.185026  25.877430  2015-07-24 00:08:03  Evenstar  Didclab-ws10  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load master data:\n",
    "import pandas as pds\n",
    "import os\n",
    "# required values :\n",
    "# OS file seperator: \n",
    "file_seperator = os.sep\n",
    "# User OS home directory:\n",
    "user_home = os.environ['HOME']\n",
    "user_home = user_home+file_seperator\n",
    "# read the master file : \n",
    "full_master_file_path = user_home + master_location + master_file_name\n",
    "data = pds.read_csv(full_master_file_path)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Patch - 1: change all Gordon oasis entries to Gordon/Oasis:\n",
    "\n",
    "data.replace(to_replace = 'Gordon/oasis',value='Gordon/Oasis',inplace = True)\n",
    "data.replace(to_replace = 'Gordon',value='Gordon/Oasis',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Evenstar', 'Didclab-ws10'),\n",
       " ('Blacklight', 'Trestles'),\n",
       " ('Stampede', 'Gordon/Oasis'),\n",
       " ('Stampede', 'Blacklight')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check - patch -1 is working :\n",
    "source_destination_name = data[['source','destination']]\n",
    "droped_duplicate_sources_destinations = source_destination_name.drop_duplicates()\n",
    "\n",
    "unique_source_destination = [tuple(sources_destinations) for sources_destinations in droped_duplicate_sources_destinations.values]\n",
    "unique_source_destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zulkar/Dropbox/gits/dataTransferScripts\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_' + current_time_str\n",
    "# final - Write data to another master file:\n",
    "with open(output_file, 'w') as f:\n",
    "        data.to_csv(f, header=True,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merger additional datasets to master.csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# provide the current master file:\n",
    "input_master_file = ''\n",
    "\n",
    "\n",
    "#linux directory:\n",
    "file_dir = '/home/zulkar/Dropbox/gits/data/ThOpt/master/'\n",
    "# mac directory: \n",
    "\n",
    "\n",
    "# output file name :\n",
    "current_date_time = datetime.datetime.now()\n",
    "current_time_str = current_date_time.strftime('%m_%d_%Y_%H_%M')\n",
    "output_file = 'master_' + current_time_str\n",
    "\n",
    "# to do : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the file :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file might not come with header information. In such case add the following line with appropriate seperator:\n",
    "# file_Size_bytes\tnumber_of_files\tbandwidth_Gbps\trtt_ms\tbuffer_size_bytes\tp\tcc\tpp\tfast\tthroughput_MByteps\ttime_sec\tstart_time_mm_dd_yyyy_hh_mm\n",
    "# read data from a csv file without reading the index : \n",
    "dd = pds.read_csv('')\n",
    "dd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Templete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# file might not come with header information. In such case add the following line with appropriate seperator:\n",
    "# file_Size_bytes\tnumber_of_files\tbandwidth_Gbps\trtt_ms\tbuffer_size_bytes\tp\tcc\tpp\tfast\tthroughput_MByteps\ttime_sec\tstart_time_mm_dd_yyyy_hh_mm\n",
    "\n",
    "full_file_name = file_dir + file_names[0]\n",
    "data = pds.read_csv(full_file_name,index_col=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "del data['Unnamed: 15']\n",
    "data['start_time'] = data['date'] +':'+ data['stime']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del data['stime']\n",
    "del data['date']\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_order = ['file_size', 'number_of_files', 'bandwidth', 'rtt', 'buffer_size', \n",
    "                'p', 'cc', 'pp', 'fast', 'throughput', 'time', 'start_time', 'source', 'destination']\n",
    "data = data[column_order]\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_miliseconds(string_list):\n",
    "    return string_list[0]\n",
    "\n",
    "temp = data.start_time.str.split('.')\n",
    "temp = temp.apply(remove_miliseconds)\n",
    "data['start_time']=temp\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert the string timestamp in dataType: Timestamp\n",
    "format_date_time = '%Y-%m-%d:%H:%M:%S'\n",
    "#format_date_time = '%m:%d:%Y-%H:%M'\n",
    "def convert_to_time_stamp(date_in_string):\n",
    "    result = pds.to_datetime(date_in_string, format=format_date_time)\n",
    "    return result\n",
    "\n",
    "# change the current string date time to Timestamp: \n",
    "data['start_time'] = data['start_time'].apply(convert_to_time_stamp)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Unit conversion:\n",
    "# file_size - MegaByte\n",
    "# bandwidth - MegaByte per second\n",
    "# buffer_size - MegaByte\n",
    "# rtt - millisecond\n",
    "# throughput - MegaByte per second\n",
    "# time - second \n",
    "# start_time - yyyy-mm-dd hh:mm:ss\n",
    "\n",
    "#d = data.copy(deep=True)\n",
    "data['file_size'] = (data['file_size'] / (1000 * 1000) ).round(decimals=2)\n",
    "data['bandwidth'] = data['bandwidth'] / 8\n",
    "data['buffer_size'] = (data['buffer_size'] / (1000 * 1000) ).round(decimals=2)\n",
    "data['rtt'] = (data['rtt'] * 1000 )\n",
    "data['throughput'] = data['throughput'] / 8 \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# write data to csv file:\n",
    "data.to_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data from a csv file without reading the index : \n",
    "dd = pds.read_csv('my_csv.csv')\n",
    "dd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# append data in an existing csv file with header false :\n",
    "with open('my_csv.csv', 'a') as f:\n",
    "    df.to_csv(f, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#d = data.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
