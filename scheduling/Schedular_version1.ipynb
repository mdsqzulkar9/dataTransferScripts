{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This schedular performs sample transfers for small files. \n",
    "# then it performs gets the local maxima from the throughput curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-requisite:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Things you need to do before running this script. \n",
    "    + Obviously choose to node with gridftp server working.\n",
    "    + Check whether globus-url-copy is working or not. \n",
    "    + Run gloubus-gridftp-sever with port in anonymous mode, or just do module load on XSEDE.\n",
    "    + You can use different ports in source and destination, however, use same port number for this script.\n",
    "    + Then run FileGenSimple.py to create the dataset or choose a dataset to transfer.\n",
    "    + Update the variables accordingly. Be consistant with FileGenSimple.py parameters. \n",
    "    + Measure RTT and bandwidth of the link, you might need those information in variable section. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys, time\n",
    "import datetime\n",
    "import math\n",
    "import logging\n",
    "import threading\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source node:  evenstar.cse.buffalo.edu\n",
      "destination node:  didclab-ws10.cse.buffalo.edu\n",
      "control port:  50000  This port is same to both source and destination, just to make life easy\n",
      "dataset path in source:  /home/zulkar/dataset/first_one/\n",
      "destination path:  /home/zulkar/received/\n",
      "link bandwidth:  1\n",
      "rtt:  0.2\n",
      "buffer size:  25600\n"
     ]
    }
   ],
   "source": [
    "# Feel free to update those variable. If you know \n",
    "# what you are doing! :-)\n",
    "\n",
    "# Script mode:\n",
    "# There are couple of modes that you can set.\n",
    "# \"experiment\" - it will perform experiments with     \n",
    "mode = \"experiment\"\n",
    "\n",
    "if mode == \"experiment\":\n",
    "    ######### VARIABLES WE WANT YOU TO SET. ######################\n",
    "    exp_name = \"exp_test\"\n",
    "    transfer_type = ['I','C']  # I - individual transfer, small or large files, \n",
    "                               # C - small and large file xfer concurrently.\n",
    "    number_of_groups = 2   # how many file types you wanna transfer as contending requests.\n",
    "    small_folder = 'sample_1_0'\n",
    "    large_folder = 'sample_3_0'\n",
    "    param_samples = [5, 10, 15, 20, 25] # uniform sampling of parameters\n",
    "    fixed_pp = 32\n",
    "    times = 5   # number of times you wanna do the transfers. 5 means it will transfer a request \n",
    "                # 5 times consequitively.\n",
    "    # Columns in the log:\n",
    "    # columns = ['exp_name','xfer_id','type','file_size','#files',\n",
    "    #           'bandwidth','rtt','buffer_size','p','cc','pp','throughput',\n",
    "    #          'time', 'source','destination']\n",
    "    # exp_log = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# node information: \n",
    "source = 'evenstar.cse.buffalo.edu'\n",
    "destination = 'didclab-ws10.cse.buffalo.edu'\n",
    "port = 50000\n",
    "source_path = \"/home/zulkar/dataset/first_one/\"\n",
    "destination_path = \"/home/zulkar/received/\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Network information: \n",
    "bandwidth = 1  # Gbps\n",
    "rtt = 0.2   # ms\n",
    "tcp_bs = 25600\n",
    "# Dataset information:\n",
    "# group range is a dictionary with key as group names:\n",
    "# group means file size range e.g. small, medium, large \n",
    "# value of this dictionary is a list:\n",
    "#      first element : lowest filesize\n",
    "#      second element : hight filesize\n",
    "#      third element : number of files\n",
    "#      fourth element : sample folders\n",
    "group_range = {'1':[1,40,100,10],'2':[100,600,50,4],'3':[900,10000,10,10]}\n",
    "ind_low  = 0\n",
    "ind_high = 1\n",
    "ind_n = 2\n",
    "ind_samples = 3\n",
    "\n",
    "\n",
    "#################### Derived variables : Don't Set those ! ################\n",
    "full_source = \"file://\" + source_path  + \"/\"\n",
    "full_destination = 'ftp://' + destination + \":\" + str(port) + destination_path\n",
    "\n",
    "buffer_size = int((bandwidth*1024*rtt*1000)/8)\n",
    "\n",
    "\n",
    "######################################################################\n",
    "\n",
    "print(\"source node: \", source)\n",
    "print(\"destination node: \", destination)\n",
    "print(\"control port: \", port, \" This port is same to both source and destination, just to make life easy\")\n",
    "print(\"dataset path in source: \", source_path)\n",
    "print(\"destination path: \", destination_path)\n",
    "print(\"link bandwidth: \", bandwidth)\n",
    "print(\"rtt: \", rtt)\n",
    "print(\"buffer size: \", buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_param_combinations(params,stype='single'):\n",
    "    \"\"\" This generates all the permutations of parameter settings. \n",
    "        Attribute : params - this is the sample list of parameters. \n",
    "                    stype  - can have two values 'single' or 'concurrent'\n",
    "                    \n",
    "        returns - list [(small_p, small_cc), (large_p, large_cc)] when stype='concurrent'\n",
    "        returns - list [(p, cc)] when stype = 'single'.\n",
    "        \"\"\"\n",
    "    single_list = [] # param combinations for single transfers.\n",
    "    for permut in itertools.permutations(params, 2):\n",
    "        single_list.append(permut)\n",
    "    \n",
    "    if stype == 'single':\n",
    "        return single_list\n",
    "    elif stype == 'concurrent':\n",
    "        con_list = []\n",
    "        for permut_c in itertools.permutations(single_list, 2):\n",
    "            #print(subset3)\n",
    "            con_list.append(permut_c)\n",
    "        return con_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lo = generate_param_combinations(param_samples,stype='concurrent')\n",
    "lo[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_folder_size(folder):\n",
    "    \"\"\" This function takes full path of a folder as input\n",
    "        Returns size of whole folder in (bytes)\n",
    "    \"\"\"\n",
    "    total_size = os.path.getsize(folder)\n",
    "    for item in os.listdir(folder):\n",
    "        itempath = os.path.join(folder, item)\n",
    "        if os.path.isfile(itempath):\n",
    "            total_size += os.path.getsize(itempath)\n",
    "        elif os.path.isdir(itempath):\n",
    "            total_size += getFolderSize(itempath)\n",
    "    return total_size # bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_file_info(folder):\n",
    "    \"\"\"This function computes total number of files and total file size.\n",
    "    input: folder - a full valid path\n",
    "    output : total_files, total_size\n",
    "    \"\"\"\n",
    "    total_size = os.path.getsize(folder)\n",
    "    for item in os.listdir(folder):\n",
    "        itempath = os.path.join(folder, item)\n",
    "        if os.path.isfile(itempath):\n",
    "            total_size += os.path.getsize(itempath)\n",
    "        elif os.path.isdir(itempath):\n",
    "            total_size += getFolderSize(itempath)\n",
    "    print(\"Total size of directory: \", total_size, \"Bytes\")        \n",
    "    total_files = 0\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        total_files += len(files)\n",
    "    print(\"Total files: \", total_files)\n",
    "    return total_files, total_size # bytes\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the total dataset size in bytes:\n",
    "n, s = get_file_info(source_path+'sample_1_0/')\n",
    "print(\"Average file size in dataset :\", s/(1024*1024), \"Mbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ls /home/zulkar/dataset/first_one/sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def experiment():\n",
    "    \"\"\" Perform data transfer experiments. In this experiment, we will discover the effect of concurrent transfers.\n",
    "    In this experiment, we group data in to two file size groups - small - large. We will see how this transfers \n",
    "    are doing under same background traffic. Assumption is data transfer didn't experience background traffic between \n",
    "    1 or 2 hours. First transfer a small file, then transfer a large file, then transfer them concurrently. \"\"\"\n",
    "    \n",
    "    from multiprocessing.pool import ThreadPool\n",
    "    pool = ThreadPool(processes=4)\n",
    "    \n",
    "    \n",
    "    # make all the permutations of parameter combinations for concurrent transfers.\n",
    "    all_permuts = generate_param_combinations(params,stype='concurrent')  \n",
    "    \n",
    "    # create the source folder path :\n",
    "    small_path_src = source_path + small_folder + '/'\n",
    "    large_path_src = source_path + large_folder + '/'\n",
    "    \n",
    "    # create the destination folder path :\n",
    "    small_path_dst = destination_path + small_folder + '/'\n",
    "    large_path_dst = destination_path + large_folder + '/'\n",
    "    \n",
    "    # full url of the source and destination folders:\n",
    "    full_small_src = \"file://\" + small_path_src  + \"/\"\n",
    "    full_large_src = \"file://\" + large_path_src  + \"/\"\n",
    "    \n",
    "    full_small_dest = 'ftp://' + destination + \":\" + str(port) + small_path_dst\n",
    "    full_large_dest = 'ftp://' + destination + \":\" + str(port) + large_path_dst\n",
    "    \n",
    "    # columns = ['exp_name','xfer_id','type','file_size','#files',\n",
    "    #           'bandwidth','rtt','buffer_size','p','cc','pp','throughput',\n",
    "    #          'time', 'source','destination']\n",
    "    \n",
    "    # list of logs where each log entry is a dictionary.\n",
    "    logs = []\n",
    "    \n",
    "    def log_gen(exp_name = 'default', transfer_id= 0,etype = 'I', file_size = 0,\n",
    "                num_files = 0, bandwidth = 0, rtt = 0, tcp_bs = 0, p = 0, cc = 0,\n",
    "                pp = 0, throughput = 0, time = 'default', source='default', destination='default' ):\n",
    "        \n",
    "        log_dict = {}\n",
    "        log_dict['exp_id'] = exp_name\n",
    "        log_dict['transfer_id'] = transfer_id\n",
    "        log_dict['type'] = etype\n",
    "    \n",
    "        log_dict['file_size'] = s\n",
    "        log_dict['num_files'] = n\n",
    "        log_dict['bandwidth'] = bandwidth\n",
    "        log_dict['rtt'] = rtt\n",
    "        log_dict['tcp_bs'] = tcp_bs\n",
    "        log_dict['p'] = p\n",
    "        log_dict['cc'] = cc\n",
    "        log_dict['pp'] = pp\n",
    "        log_dict['throughput'] = throughput\n",
    "        log_dict['time'] = time\n",
    "        log_dict['source'] = source \n",
    "        log_dict['destination'] = destination\n",
    "        \n",
    "        return log_dict\n",
    "        \n",
    "        \n",
    "    for idx, permut in enumerate(all_permuts):\n",
    "        small_p = permut[0][0]\n",
    "        small_cc = permut[0][1]\n",
    "        small_pp = fixed_pp\n",
    "        \n",
    "        large_p = permut[1][0]\n",
    "        large_cc = permut[1][1]\n",
    "        large_pp = fixed_pp\n",
    "        \n",
    "        # perform small transfer:\n",
    "        small_time, small_th = perform_transfer(full_small_src, full_small_dest, p = small_p, cc=small_cc, \n",
    "                                                pp=small_pp, fast=True, tcp_bs=tcp_bs)\n",
    "        \n",
    "        \n",
    "        n,s = get_file_info(small_path)\n",
    "        \n",
    "        log = log_gen(exp_name = exp_name, transfer_id= idx,etype = 'I', file_size = s,\n",
    "                num_files = n, bandwidth = bandwidth, rtt = rtt, tcp_bs = tcp_bs, p = small_p, cc = small_cc,\n",
    "                pp = small_pp, throughput = small_th, time = small_time, source=source, destination=destination )\n",
    "        \n",
    "        logs.append(log)\n",
    "        \n",
    "        \n",
    "        # perform large transfer:\n",
    "        large_time, large_th = perform_transfer(full_large_src, full_large_dest, p = large_p, cc=large_cc, \n",
    "                                                pp=large_pp, fast=True, tcp_bs=tcp_bs)\n",
    "        \n",
    "        n,s = get_file_info(large_path)\n",
    "        log = log_gen(exp_name = exp_name, transfer_id= idx,etype = 'I', file_size = s,\n",
    "                num_files = n, bandwidth = bandwidth, rtt = rtt, tcp_bs = tcp_bs, p = large_p, cc = large_cc,\n",
    "                pp = large_pp, throughput = large_th, time = large_time, source=source, destination=destination )\n",
    "        \n",
    "        logs.append(log)\n",
    "        \n",
    "        \n",
    "        # perform small and large transfer:\n",
    "    \n",
    "        results = []\n",
    "        for i in range(5):\n",
    "            async_result = pool.apply_async(perform_transfer, (source, destination)) # tuple of args for foo\n",
    "            #async_result = pool.apply_async(foo, ('world', 'foo')) # tuple of args for foo\n",
    "            results.append(async_result)\n",
    "        # do some other stuff in the main process\n",
    "\n",
    "        for r in results:\n",
    "            return_val = r.get()  # get the return value from your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing.pool import ThreadPool\n",
    "pool = ThreadPool(processes=4)\n",
    "results = []\n",
    "for i in range(5):\n",
    "    async_result = pool.apply_async(foo, ('world', 'foo')) # tuple of args for foo\n",
    "    results.append(async_result)\n",
    "# do some other stuff in the main process\n",
    "\n",
    "for r in results:\n",
    "    return_val = r.get()  # get the return value from your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_transfer(source, destination, p = 1, cc=1, pp=1, fast=True, tcp_bs=25600):\n",
    "    \"\"\"To do a transfer you need to provide:\n",
    "            source \n",
    "            destination\n",
    "            p\n",
    "            cc\n",
    "            pp\n",
    "            fast\n",
    "            tcp-bs\n",
    "       performs the transfer and returns \n",
    "           throughput\n",
    "           time\n",
    "    \"\"\"\n",
    "    space = \" \"\n",
    "    globus_cmd = \"globus-url-copy\" + \\\n",
    "                    space + \"-tcp-bs\" + space + str(tcp_bs) + \\\n",
    "                    space + \"-p\" + space + str(p) + \\\n",
    "                    space + \"-cc\" + space + str(cc) + \\\n",
    "                    space + \"-pp\" + space + str(pp) + \\\n",
    "                    space + full_source + \\\n",
    "                    space + full_destination\n",
    "    globus_cmd \n",
    "    \n",
    "    print(\"started data transfer: \")\n",
    "    print(\"globus command:\",globus_cmd)\n",
    "    \n",
    "    \n",
    "    transfer_start_time = time.time()\n",
    "    os.system(globus_cmd)\n",
    "    transfer_finish_time = time.time()  # in seconds\n",
    "    \n",
    "    time_required = transfer_finish_time - transfer_start_time \n",
    "    bytes_xfered = getFolderSize(sample_path)\n",
    "    \n",
    "    \n",
    "    throughput = ( bytes_xfered *8) / (time_required * 1024 * 1024 )     # in Giga bit per seconds\n",
    "    throughput\n",
    "    \n",
    "    print(\"transfer finished !\")\n",
    "    print(\"bytes transferred: \", bytes_xfered)\n",
    "    print(\"time needed for transfer: \", time_required, \"s.\")\n",
    "    print(\"throughput: \", throughput)\n",
    "    \n",
    "    return time_required, throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def foo(bar, baz):\n",
    "    n = datetime.datetime.now()\n",
    "    #print('hello {0}'.format(bar))\n",
    "    print(n)\n",
    "    return 'foo' + baz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def worker(num):\n",
    "    \"\"\"thread worker function\"\"\"\n",
    "    print ('Worker: %s' % num)\n",
    "    return\n",
    "\n",
    "threads = []\n",
    "for i in range(5):\n",
    "    t = threading.Thread(target=worker, args=(i,))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import threading\n",
    "import time\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='[%(levelname)s] (%(threadName)-10s) %(message)s',\n",
    "                    )\n",
    "\n",
    "def worker():\n",
    "    logging.debug('Starting')\n",
    "    time.sleep(2)\n",
    "    logging.debug('Exiting')\n",
    "\n",
    "def my_service():\n",
    "    logging.debug('Starting')\n",
    "    time.sleep(3)\n",
    "    logging.debug('Exiting')\n",
    "\n",
    "t = threading.Thread(name='my_service', target=my_service)\n",
    "w = threading.Thread(name='worker', target=worker)\n",
    "w2 = threading.Thread(target=worker) # use default name\n",
    "\n",
    "w.start()\n",
    "w2.start()\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def foo(bar, baz):\n",
    "  print('hello {0}'.format(bar))\n",
    "  return 'foo' + baz\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "pool = ThreadPool(processes=1)\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    async_result = pool.apply_async(foo, ('world', 'foo')) # tuple of args for foo\n",
    "    results.append(async_result)\n",
    "# do some other stuff in the main process\n",
    "\n",
    "for r in results:\n",
    "    return_val = r.get()  # get the return value from your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "stuff = [ 5, 10, 15, 20, 25]\n",
    "\n",
    "list1 = []\n",
    "for subset1 in itertools.permutations(stuff, 2):\n",
    "    #print(subset1)\n",
    "    list1.append(subset1)\n",
    "list2 = []\n",
    "for subset2 in itertools.permutations(stuff, 2):\n",
    "    #print(subset2)\n",
    "    list2.append(subset2)\n",
    "list3 = []\n",
    "for subset3 in itertools.permutations(list1, 2):\n",
    "    #print(subset3)\n",
    "    list3.append(subset3)\n",
    "len(list3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Schedular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if mode == \"experiment\":\n",
    "    do_experiment():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform initial transfer from small files.\n",
    "# group_1 is the small files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize sample data structure for samples to be transferred.\n",
    "# its a three level deep dictionary:\n",
    "# level one : map - group_id to sample_id\n",
    "# level two : map - sample_id to sample_fields\n",
    "# level three : map sample_fields to corresponding values\n",
    "# sample_info = { <group_id> : { <sample_id> : { 'path': <path_val>,\n",
    "#                                         'size': <samplesize in bytes>,\n",
    "#                                         'transferred': <transferred_or_not boolean>,\n",
    "#                                         'throughput' : <throughput_val in Gbps>,\n",
    "#                                         'transfer-time' : <val>,\n",
    "#                                         'p' : <p_value>,\n",
    "#                                         'cc' : <cc_value>,\n",
    "#                                         'pp' : <pp_value>,\n",
    "#                                         'fast' : <fast value>,\n",
    "#                                         'tcp-bs': <tcp_buffer size>,\n",
    "#                                         'traffic-intensity': <intensity from 0 to 1>}}}\n",
    "\n",
    "sample_info = {}\n",
    "for key, value in group_range.items():\n",
    "    group_id = key\n",
    "    n_samples = value[ind_samples]\n",
    "    \n",
    "    level_two_dict = {}\n",
    "    for sample in range(n_samples):\n",
    "        level_three_dict = {}\n",
    "        \n",
    "        # Compute path:\n",
    "        sample_folder_name = 'sample_' + group_id + '_' + str(sample)\n",
    "        full_sample_path = source_path + sample_folder_name\n",
    "        \n",
    "        level_three_dict['path'] = full_sample_path\n",
    "        \n",
    "        # Compute size:\n",
    "        level_three_dict['size'] = getFolderSize(full_sample_path)\n",
    "        \n",
    "        level_three_dict['transferred'] = False\n",
    "        \n",
    "        level_three_dict['p'] = 0\n",
    "        \n",
    "        level_three_dict['cc'] = 0\n",
    "        \n",
    "        level_three_dict['pp'] = 0\n",
    "        \n",
    "        level_three_dict['fast'] = 'false'\n",
    "        \n",
    "        level_three_dict['tcp-bs'] = 25600\n",
    "        \n",
    "        level_three_dict['throughput'] = 0\n",
    "        \n",
    "        level_three_dict['transfer-time'] = 0 \n",
    "        \n",
    "        \n",
    "        # Add level three dict to level two dict\n",
    "        sample_name = str(sample)\n",
    "        level_two_dict[sample_name] = level_three_dict\n",
    "    \n",
    "    # Add level two dict to level one dict  (sample_info)  \n",
    "    sample_info[group_id] = level_two_dict\n",
    "#sample_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_info['1']['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the total dataset size in bytes:\n",
    "dataset_size = getFolderSize(source_path)\n",
    "print(\"Total size of dataset :\", dataset_size/(1024*1024), \"Mbytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get information of all small samples:\n",
    "group_id = '1'\n",
    "sample_id = '0'\n",
    "\n",
    "# get local \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from multiprocessing import Pool, freeze_support\n",
    "\n",
    "def func(a, b):\n",
    "    return a + b\n",
    "\n",
    "def main():\n",
    "    a_args = [1,2,3]\n",
    "    second_arg = 1\n",
    "    with Pool() as pool:\n",
    "        L = pool.starmap(func, [(1, 1), (2, 1), (3, 1)])\n",
    "        M = pool.starmap(func, zip(a_args, repeat(second_arg)))\n",
    "        N = pool.map(partial(func, b=second_arg), a_args)\n",
    "        assert L == M == N\n",
    "\n",
    "\n",
    "freeze_support()\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    " \n",
    "def doubler(number):\n",
    "    return number * 2\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    numbers = [5, 10, 20]\n",
    "    pool = Pool(processes=3)\n",
    "    print(pool.map(doubler, numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    " \n",
    "def doubler(number):\n",
    "    n = datetime.datetime.now()\n",
    "    time.sleep(4)\n",
    "    return n\n",
    " \n",
    "\n",
    "pool = Pool(processes=5)\n",
    "for i in range(5):\n",
    "    result = pool.map(doubler, (25,))\n",
    "    print(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Lock\n",
    "\n",
    "def func1():\n",
    "    n = datetime.datetime.now()\n",
    "    print ('func1: starting: ',n )\n",
    "    \n",
    "    \n",
    "    \n",
    "    n = datetime.datetime.now()\n",
    "    print ('func1: finishing :',n)\n",
    "    return 'hello'\n",
    "def func2():\n",
    "    n = datetime.datetime.now()\n",
    "    print ('func2: starting: ', n)\n",
    "    time.sleep(3)\n",
    "    n = datetime.datetime.now()\n",
    "    print ('func2: finishing: ', n)\n",
    "\n",
    "\n",
    "p1 = Process(target=func1,args=('hi'))\n",
    "p1.start()\n",
    "p2 = Process(target=func2)\n",
    "p2.start()\n",
    "p1.join()\n",
    "p2.join()\n",
    "print(\"I am here!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l.acquire()\n",
    "try:\n",
    "    print('hello world', i)\n",
    "finally:\n",
    "    l.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Right one for multi-threading : \n",
    "\n",
    "\n",
    "import datetime\n",
    "n1 = datetime.datetime.now()\n",
    "print(n1)\n",
    "thread_return1={'success': False, 'result':n1}\n",
    "thread_return2={'success': False, 'result':n1}\n",
    "\n",
    "from threading import Thread\n",
    "def task(thread_return,tk):\n",
    "    time.sleep(tk)\n",
    "    n = datetime.datetime.now()\n",
    "    thread_return['success'] = True\n",
    "    thread_return['result'] = n\n",
    "t1 = Thread(target=task, args=(thread_return1,10))\n",
    "t2 = Thread(target=task, args=(thread_return2,4))\n",
    "\n",
    "t1.start()\n",
    "t2.start()\n",
    "\n",
    "t1.join()\n",
    "t2.join()\n",
    "\n",
    "print(thread_return1)\n",
    "print(thread_return2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:power_env]",
   "language": "python",
   "name": "conda-env-power_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
